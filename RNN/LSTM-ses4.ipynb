{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAJGLF63_gX-",
        "outputId": "4d8b131f-6c5e-49a6-85ed-bd7f6edb887f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-WFjYIDN_o71"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rzgp0fzhPtkg"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RxXg7fV_q2tn"
      },
      "outputs": [],
      "source": [
        "class DataReader:\n",
        "  def __init__(self, data_path, batch_size, vocab_size):\n",
        "    self.batch_size = batch_size\n",
        "    with open(data_path) as f:\n",
        "      d_lines = f.read().splitlines()\n",
        "    \n",
        "    self.data = []\n",
        "    self.labels = []\n",
        "    self.sentence_lengths = []\n",
        "    # self.final_tokens = []\n",
        "    for line in d_lines:\n",
        "      # vector = [0. for i in range(vocab_size)]\n",
        "      features = line.split('<fff>')\n",
        "      label, doc_id, length = int(features[0]), int(features[1]), int(features[2])\n",
        "      tokens = features[3].split()\n",
        "      for token in tokens:\n",
        "        token = int(token)\n",
        "      self.data.append(tokens)\n",
        "      self.labels.append(label)\n",
        "      self.sentence_lengths.append(length)\n",
        "      # self.final_tokens.append(tokens[len(tokens)-1])\n",
        "    self.data = np.array(self.data)\n",
        "    self.labels = np.array(self.labels)\n",
        "    self.sentence_lengths = np.array(self.sentence_lengths)\n",
        "    # self.final_tokens = np.array(self.final_tokens)\n",
        "\n",
        "    self.num_epoch = 0\n",
        "    self.batch_id = 0\n",
        "  \n",
        "  def next_batch(self):\n",
        "    start = self.batch_id * self.batch_size\n",
        "    end = start + self.batch_size\n",
        "    self.batch_id += 1\n",
        "\n",
        "    if end + self.batch_size > len(self.data):\n",
        "      # end = len(self.data)\n",
        "      self.num_epoch += 1\n",
        "      self.batch_id = 0\n",
        "      indices = np.arange(len(self.data))\n",
        "      np.random.seed(2018)\n",
        "      np.random.shuffle(indices)\n",
        "      self.data, self.labels = self.data[indices], self.labels[indices]\n",
        "      self.sentence_lengths  = self.sentence_lengths[indices]\n",
        "      # self.final_tokens = self.final_tokens[indices]\n",
        "\n",
        "    return self.data[start:end], self.labels[start:end], self.sentence_lengths[start:end]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuUQWuDL_rbi",
        "outputId": "01f3ae35-d2e1-479e-e1c8-5ea9c3a7e3bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-5-391d9b5ebd15>:59: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-5-391d9b5ebd15>:67: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-5-391d9b5ebd15>:34: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "loss: 1.2252155e-05\n",
            "loss: 1.1174552\n",
            "loss: 5.477728\n",
            "loss: 6.833845e-05\n",
            "loss: 0.0006010126\n",
            "loss: 4.5330067\n",
            "loss: 4.279049\n",
            "loss: 0.30166486\n",
            "loss: 4.7344313\n",
            "loss: 10.744032\n",
            "loss: 7.2799096\n",
            "Epoch: 1\n",
            "Accuracy on test data: 2.257036643653744\n",
            "loss: 3.772585\n",
            "loss: 3.378634\n",
            "loss: 2.906875\n",
            "loss: 2.6080594\n",
            "loss: 2.3996415\n",
            "loss: 2.2487533\n",
            "loss: 2.1863325\n",
            "loss: 2.3342688\n",
            "loss: 1.8987343\n",
            "loss: 1.8009939\n",
            "loss: 1.9003273\n",
            "Epoch: 2\n",
            "Accuracy on test data: 44.03876792352629\n",
            "loss: 1.5989258\n",
            "loss: 1.6889073\n",
            "loss: 1.3625275\n",
            "loss: 1.4797566\n",
            "loss: 1.5211482\n",
            "loss: 1.5611374\n",
            "loss: 1.074804\n",
            "loss: 1.3076532\n",
            "loss: 1.1613646\n",
            "loss: 1.4445682\n",
            "loss: 1.2611301\n",
            "Epoch: 3\n",
            "Accuracy on test data: 54.766330323951145\n",
            "loss: 0.97466993\n",
            "loss: 1.2745548\n",
            "loss: 0.8427465\n",
            "loss: 1.037558\n",
            "loss: 1.048053\n",
            "loss: 1.2085019\n",
            "loss: 0.8463653\n",
            "loss: 1.1197602\n",
            "loss: 0.7333492\n",
            "loss: 0.97232467\n",
            "loss: 1.1398247\n",
            "loss: 0.88637924\n",
            "Epoch: 4\n",
            "Accuracy on test data: 59.25385023898035\n",
            "loss: 0.77707475\n",
            "loss: 0.67392725\n",
            "loss: 0.6318467\n",
            "loss: 0.91860753\n",
            "loss: 0.57881373\n",
            "loss: 0.90249556\n",
            "loss: 0.57937205\n",
            "loss: 0.9593779\n",
            "loss: 0.8915686\n",
            "loss: 0.71824914\n",
            "loss: 0.7748555\n",
            "Epoch: 5\n",
            "Accuracy on test data: 59.97079129049389\n",
            "loss: 0.7207208\n",
            "loss: 0.6558832\n",
            "loss: 0.6986533\n",
            "loss: 0.6179972\n",
            "loss: 0.6648841\n",
            "loss: 0.5929607\n",
            "loss: 0.64363503\n",
            "loss: 0.7431286\n",
            "loss: 0.5335855\n",
            "loss: 0.62426084\n",
            "loss: 1.05215\n",
            "Epoch: 6\n",
            "Accuracy on test data: 60.72756240042485\n",
            "loss: 0.477297\n",
            "loss: 0.79647475\n",
            "loss: 0.52594346\n",
            "loss: 0.8001889\n",
            "loss: 0.54505134\n",
            "loss: 0.7639683\n",
            "loss: 0.54565924\n",
            "loss: 0.64603484\n",
            "loss: 0.7300327\n",
            "loss: 0.692227\n",
            "loss: 0.81121016\n",
            "loss: 0.68375635\n",
            "Epoch: 7\n",
            "Accuracy on test data: 61.032926181625065\n",
            "loss: 0.6060018\n",
            "loss: 0.50003415\n",
            "loss: 0.3897956\n",
            "loss: 0.5564381\n",
            "loss: 0.9996041\n",
            "loss: 0.6166666\n",
            "loss: 0.5393306\n",
            "loss: 0.37418234\n",
            "loss: 0.68573546\n",
            "loss: 0.51742476\n",
            "loss: 0.74965554\n",
            "Epoch: 8\n",
            "Accuracy on test data: 61.93574083908656\n",
            "loss: 0.39539266\n",
            "loss: 0.774837\n",
            "loss: 0.41186866\n",
            "loss: 0.48942485\n",
            "loss: 0.5529718\n",
            "loss: 0.3667861\n",
            "loss: 0.5428611\n",
            "loss: 0.4878547\n",
            "loss: 0.64503074\n",
            "loss: 0.4996703\n",
            "loss: 0.36003184\n",
            "Epoch: 9\n",
            "Accuracy on test data: 61.61710037174721\n",
            "loss: 0.30453467\n",
            "loss: 0.2927775\n",
            "loss: 0.47398072\n",
            "loss: 0.34722903\n",
            "loss: 0.51282275\n",
            "loss: 0.25923452\n",
            "loss: 0.30354154\n",
            "loss: 0.7354868\n",
            "loss: 0.63154745\n",
            "loss: 0.72384703\n",
            "loss: 0.6300375\n",
            "loss: 0.62113994\n",
            "Epoch: 10\n",
            "Accuracy on test data: 60.35581518852894\n",
            "loss: 0.6906979\n",
            "loss: 0.44367608\n",
            "loss: 0.41842583\n",
            "loss: 0.3612082\n",
            "loss: 0.60302633\n",
            "loss: 0.50463986\n",
            "loss: 0.36221093\n",
            "loss: 0.5073923\n",
            "loss: 0.50511897\n",
            "loss: 0.95720595\n",
            "loss: 0.560619\n",
            "Epoch: 11\n",
            "Accuracy on test data: 62.851832182687204\n",
            "loss: 0.46074536\n",
            "loss: 0.24907927\n",
            "loss: 0.3072191\n",
            "loss: 0.5062736\n",
            "loss: 0.31217396\n",
            "loss: 0.33233374\n",
            "loss: 0.38838822\n",
            "loss: 0.4026012\n",
            "loss: 0.49955937\n",
            "loss: 0.875598\n",
            "loss: 0.5048262\n",
            "Epoch: 12\n",
            "Accuracy on test data: 61.829527349973446\n",
            "loss: 0.3279261\n",
            "loss: 0.24954662\n",
            "loss: 0.53499836\n",
            "loss: 0.33155772\n",
            "loss: 0.2907745\n",
            "loss: 0.39759937\n",
            "loss: 0.66428393\n",
            "loss: 0.4139354\n",
            "loss: 0.36506873\n",
            "loss: 0.5545279\n",
            "loss: 0.41082028\n",
            "Epoch: 13\n",
            "Accuracy on test data: 62.12161444503452\n",
            "loss: 0.37345174\n",
            "loss: 0.6387422\n",
            "loss: 0.50324386\n",
            "loss: 0.46598566\n",
            "loss: 0.67435044\n",
            "loss: 0.8574068\n",
            "loss: 0.4964143\n",
            "loss: 0.50421274\n",
            "loss: 0.33201087\n",
            "loss: 0.6895803\n",
            "loss: 0.4065353\n",
            "loss: 0.6168921\n",
            "Epoch: 14\n",
            "Accuracy on test data: 62.51991502920871\n",
            "loss: 0.24833074\n",
            "loss: 0.31123146\n",
            "loss: 0.28646338\n",
            "loss: 0.5441802\n",
            "loss: 0.4518586\n",
            "loss: 0.5840263\n",
            "loss: 0.41660297\n",
            "loss: 0.44542068\n",
            "loss: 0.3671123\n",
            "loss: 0.42225984\n",
            "loss: 0.5580332\n",
            "Epoch: 15\n",
            "Accuracy on test data: 62.798725438130646\n",
            "loss: 0.45063537\n",
            "loss: 0.35498616\n",
            "loss: 0.2010351\n",
            "loss: 0.67509496\n",
            "loss: 0.30701715\n",
            "loss: 0.4283999\n",
            "loss: 0.4395285\n",
            "loss: 0.37719238\n",
            "loss: 0.7710117\n",
            "loss: 0.40708587\n",
            "loss: 0.43699765\n",
            "Epoch: 16\n",
            "Accuracy on test data: 63.25013276686139\n",
            "loss: 0.49454585\n",
            "loss: 0.29912978\n",
            "loss: 0.36045167\n",
            "loss: 0.5583644\n",
            "loss: 0.72080445\n",
            "loss: 0.34084976\n",
            "loss: 0.49914864\n",
            "loss: 0.3607834\n",
            "loss: 0.7236389\n",
            "loss: 0.25429544\n",
            "loss: 0.32905647\n",
            "loss: 0.26311913\n",
            "Epoch: 17\n",
            "Accuracy on test data: 63.56877323420074\n",
            "loss: 0.2869935\n",
            "loss: 0.15107463\n",
            "loss: 0.461146\n",
            "loss: 0.57769\n",
            "loss: 0.48034665\n",
            "loss: 0.3270663\n",
            "loss: 0.28942597\n",
            "loss: 0.39172184\n",
            "loss: 0.34339526\n",
            "loss: 0.3317005\n",
            "loss: 0.5354268\n",
            "Epoch: 18\n",
            "Accuracy on test data: 63.090812533191716\n",
            "loss: 0.24195714\n",
            "loss: 0.29469624\n",
            "loss: 0.2519076\n",
            "loss: 0.7091678\n",
            "loss: 0.32371303\n",
            "loss: 0.40392834\n",
            "loss: 0.35179183\n",
            "loss: 0.57889485\n",
            "loss: 0.5813205\n",
            "loss: 0.48748356\n",
            "loss: 0.46344882\n",
            "Epoch: 19\n",
            "Accuracy on test data: 62.851832182687204\n",
            "loss: 0.37971374\n",
            "loss: 0.18511368\n",
            "loss: 0.24476868\n",
            "loss: 0.20062214\n",
            "loss: 0.33874637\n",
            "loss: 0.29331756\n",
            "loss: 0.42734882\n",
            "loss: 0.32714924\n",
            "loss: 0.6566862\n",
            "loss: 0.23969813\n",
            "loss: 0.38889968\n",
            "loss: 0.2633875\n",
            "Epoch: 20\n",
            "Accuracy on test data: 61.89591078066915\n",
            "loss: 0.24496321\n",
            "loss: 0.16503397\n",
            "loss: 0.304045\n",
            "loss: 0.5100368\n",
            "loss: 0.36491245\n",
            "loss: 0.24597654\n",
            "loss: 0.37976828\n",
            "loss: 0.46027592\n",
            "loss: 0.41740346\n",
            "loss: 0.5336844\n",
            "loss: 0.41949272\n",
            "Epoch: 21\n",
            "Accuracy on test data: 61.64365374402549\n",
            "loss: 0.3869848\n",
            "loss: 0.5356162\n",
            "loss: 0.2423038\n",
            "loss: 0.23582263\n",
            "loss: 0.79471666\n",
            "loss: 0.28098908\n",
            "loss: 0.6843702\n",
            "loss: 0.5813833\n",
            "loss: 0.30642617\n",
            "loss: 0.65567946\n",
            "loss: 0.38893458\n",
            "Epoch: 22\n",
            "Accuracy on test data: 60.78066914498141\n",
            "loss: 0.35142207\n",
            "loss: 0.31453994\n"
          ]
        }
      ],
      "source": [
        "class RNN:\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.lstm_size = lstm_size\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.MAX_DOC_LENGTHS = 500\n",
        "    self.data = tf.placeholder(tf.int32, shape=[batch_size, self.MAX_DOC_LENGTHS])\n",
        "    self.labels = tf.placeholder(tf.int32, shape=[batch_size,])\n",
        "    self.sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size,])\n",
        "    # self.final_tokens = tf.placeholder(tf.int32, shape=[batch_size,])\n",
        "\n",
        "  def build_graph(self):\n",
        "    NUM_CLASSES = 20\n",
        "    embeddings = self.embedding_layer(self.data)\n",
        "\n",
        "    lstm_outputs = self.LSTM_layer(embeddings)\n",
        "\n",
        "    weights = tf.get_variable(name='final_layer_weights', \n",
        "                              shape=(self.lstm_size, NUM_CLASSES),\n",
        "                              initializer=tf.random_normal_initializer(seed=2018))\n",
        "    biases = tf.get_variable(name='final_layer_biases',\n",
        "                             shape=(NUM_CLASSES),\n",
        "                             initializer=tf.random_normal_initializer(seed=2018))\n",
        "    logits = tf.matmul(lstm_outputs, weights) + biases\n",
        "\n",
        "    labels_one_hot = tf.one_hot(indices=self.labels,\n",
        "                                depth=NUM_CLASSES,\n",
        "                                dtype=tf.float32\n",
        "                                )\n",
        "\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot,\n",
        "                                                   logits=logits)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "\n",
        "    probs = tf.nn.softmax(logits)\n",
        "\n",
        "    predicted_labels = tf.argmax(probs, axis=1)\n",
        "    predicted_labels = tf.squeeze(predicted_labels)\n",
        "    \n",
        "    return predicted_labels, loss\n",
        "  def embedding_layer(self, indices):\n",
        "    pretrained_vectors = [np.zeros(self.embedding_size)]\n",
        "    np.random.seed(2018)\n",
        "\n",
        "    for _ in range(self.vocab_size + 1):\n",
        "      pretrained_vectors.append(np.random.normal(loc=0, scale=1., size=self.embedding_size))\n",
        "    \n",
        "    pretrained_vectors = np.array(pretrained_vectors)\n",
        "\n",
        "    self.embedding_matrix = tf.get_variable(name='embedding', \n",
        "                                            shape=(self.vocab_size+2, self.embedding_size),\n",
        "                                            initializer=tf.constant_initializer(pretrained_vectors))\n",
        "    \n",
        "    return tf.nn.embedding_lookup(self.embedding_matrix, indices)\n",
        "  \n",
        "  def LSTM_layer(self, embeddings):\n",
        "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.lstm_size)\n",
        "    zero_state = tf.zeros(shape=(self.batch_size, self.lstm_size))\n",
        "    initial_state = tf.contrib.rnn.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "    lstm_inputs = tf.unstack(tf.transpose(embeddings, perm=[1,0,2]))\n",
        "    lstm_outputs, last_state = tf.nn.static_rnn(cell=lstm_cell, \n",
        "                                                inputs=lstm_inputs,\n",
        "                                                initial_state=initial_state,\n",
        "                                                sequence_length=self.sentence_lengths)\n",
        "    lstm_outputs = tf.unstack(tf.transpose(lstm_outputs, perm=[1,0,2]))\n",
        "    lstm_outputs = tf.concat(lstm_outputs, axis=0)\n",
        "\n",
        "    mask = tf.sequence_mask(lengths=self.sentence_lengths,\n",
        "                            maxlen=self.MAX_DOC_LENGTHS,\n",
        "                            dtype=tf.float32)\n",
        "    mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
        "    mask = tf.expand_dims(mask, -1)\n",
        "\n",
        "    lstm_outputs = mask*lstm_outputs\n",
        "    lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self.batch_size)\n",
        "    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n",
        "    lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(tf.cast(self.sentence_lengths, tf.float32), -1)\n",
        "\n",
        "    return lstm_outputs_average\n",
        "\n",
        "  def trainer(self, loss, learning_rate):\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "    return train_op\n",
        "\n",
        "with open('vocab-raw.txt', 'rb') as f:\n",
        "  vocab_size = len(f.read().splitlines())\n",
        "\n",
        "tf.set_random_seed(2018)\n",
        "rnn = RNN(vocab_size=vocab_size,\n",
        "            embedding_size=50,\n",
        "            lstm_size=50,\n",
        "            batch_size=50)\n",
        "\n",
        "predicted_labels, loss = rnn.build_graph()\n",
        "train_op = rnn.trainer(loss=loss, learning_rate=0.1)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  train_data_reader = DataReader(data_path='20news-train-encoded-v2.txt', batch_size=50, vocab_size=vocab_size)\n",
        "  test_data_reader = DataReader(data_path='20news-test-encoded-v2.txt', batch_size=50, vocab_size=vocab_size)\n",
        "\n",
        "  step = 0\n",
        "  MAX_STEP = 5000\n",
        "    \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  while step < MAX_STEP:\n",
        "    next_train_batch = train_data_reader.next_batch()\n",
        "    train_data, train_labels, train_sentence_lengths = next_train_batch  \n",
        "    plabels_eval, loss_eval, _ = sess.run([predicted_labels, loss, train_op],\n",
        "                                            feed_dict={\n",
        "                                                rnn.data: train_data,\n",
        "                                                rnn.labels: train_labels,\n",
        "                                                rnn.sentence_lengths: train_sentence_lengths\n",
        "                                                \n",
        "                                            })\n",
        "    step+=1\n",
        "    if step % 20 == 0:\n",
        "      print(\"loss:\", loss_eval)\n",
        "   \n",
        "    if train_data_reader.batch_id == 0:\n",
        "      num_true_preds = 0\n",
        "      while True:\n",
        "        next_test_batch = test_data_reader.next_batch()\n",
        "        test_data, test_labels, test_sentence_lengths = next_test_batch\n",
        "\n",
        "        test_plabels_eval = sess.run(predicted_labels,\n",
        "                                       feed_dict={\n",
        "                                           rnn.data: test_data,\n",
        "                                           rnn.labels: test_labels,\n",
        "                                           rnn.sentence_lengths: test_sentence_lengths\n",
        "                                          \n",
        "                                       })\n",
        "        matches = np.equal(test_plabels_eval, test_labels)\n",
        "        num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "        if test_data_reader.batch_id == 0:\n",
        "          break\n",
        "      print('Epoch:', train_data_reader.num_epoch)\n",
        "      print('Accuracy on test data:', num_true_preds * 100. / len(test_data_reader.data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UQbSRThNUA1o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S22UlU9wQx8z"
      },
      "outputs": [],
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s08_MyIWIZi4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
