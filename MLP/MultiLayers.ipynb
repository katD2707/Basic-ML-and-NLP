{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqjjhxYwwP2p",
        "outputId": "4a880d16-8bce-40bb-d2f0-b3aec8f4da3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "# tf.disable_v2_behavior()\n",
        "# from tensorflow.python.keras import Sequential\n",
        "# from tensorflow.python.keras.layers import Dense, Lambda\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "# import keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uUho3ouYqAU",
        "outputId": "16d09497-940e-4dd4-b57c-8cb6f805ee5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VWfk4uB1wcyB"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, vocab_size, hidden_size, num_classes):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "    def build_graph(self):\n",
        "        self.X = tf.placeholder(tf.float32, shape=[None, self.vocab_size])\n",
        "        self.y_real = tf.placeholder(tf.int32, shape=[None,])\n",
        "        weight_1 = tf.get_variable(name='weight_input_hidden', \n",
        "                         shape=(self.vocab_size, self.hidden_size),\n",
        "                         initializer=tf.random_normal_initializer(seed=2018))\n",
        "        biases_1 = tf.get_variable(name='biases_input_hidden', \n",
        "                               shape=(self.hidden_size),\n",
        "                               initializer=tf.random_normal_initializer(seed=2018))\n",
        "        weight_2 = tf.get_variable(name='weight_hidden_output', \n",
        "                         shape=(self.hidden_size, self.num_classes),\n",
        "                         initializer=tf.random_normal_initializer(seed=2018))\n",
        "        biases_2 = tf.get_variable(name='biases_hidden_output', \n",
        "                               shape=(self.num_classes),\n",
        "                               initializer=tf.random_normal_initializer(seed=2018))\n",
        "        \n",
        "        hidden = tf.matmul(self.X, weight_1) + biases_1\n",
        "        hidden = tf.sigmoid(hidden)\n",
        "\n",
        "        logits = tf.matmul(hidden, weight_2) + biases_2\n",
        "\n",
        "        labels_one_hot = tf.one_hot(indices=self.y_real, depth=self.num_classes, dtype=tf.float32)\n",
        "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
        "   \n",
        "        loss = tf.reduce_mean(loss)\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        predicted_labels = tf.argmax(probs, axis=1)\n",
        "        predicted_labels = tf.squeeze(predicted_labels)\n",
        "\n",
        "        return predicted_labels, loss\n",
        "      \n",
        "    def trainer(self, loss, learning_rate):\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "        return train_op\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DsvyWPh1xOqc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class DataReader:\n",
        "  def __init__(self, data_path, batch_size, vocab_size):\n",
        "    self.batch_size = batch_size\n",
        "    with open(data_path) as f:\n",
        "      d_lines = f.read().splitlines()\n",
        "    \n",
        "    self.data = []\n",
        "    self.labels = []\n",
        "    for data_id, line in enumerate(d_lines):\n",
        "      vector = [0. for i in range(vocab_size)]\n",
        "      features = line.split('<fff>')\n",
        "      label, doc_id = int(features[0]), int(features[1])\n",
        "      tokens = features[2].split()\n",
        "      for token in tokens:\n",
        "        index, value = int(token.split(':')[0]), float(token.split(':')[1])\n",
        "        vector[index] = value\n",
        "      self.data.append(vector)\n",
        "      self.labels.append(label)\n",
        "    \n",
        "    self.data = np.array(self.data)\n",
        "    self.labels = np.array(self.labels)\n",
        "\n",
        "    self.num_epoch = 0\n",
        "    self.batch_id = 0\n",
        "  \n",
        "  def next_batch(self):\n",
        "    start = self.batch_id * self.batch_size\n",
        "    end = start + self.batch_size\n",
        "    self.batch_id += 1\n",
        "\n",
        "    if end + self.batch_size > len(self.data):\n",
        "      end = len(self.data)\n",
        "      self.num_epoch += 1\n",
        "      self.batch_id = 0\n",
        "      indices = np.arange(len(self.data))\n",
        "      np.random.seed(2018)\n",
        "      np.random.shuffle(indices)\n",
        "      self.data, self.labels = self.data[indices], self.labels[indices]\n",
        "\n",
        "    return self.data[start:end], self.labels[start:end]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFtp0_jZ9c_8",
        "outputId": "5792a8ad-0ab7-4fda-dbea-7127ccc4c733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-1533e2813a37>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('word_idfs.txt', 'rb') as f:\n",
        "  vocab_size = len(f.read().splitlines())\n",
        "\n",
        "mlp = MLP(vocab_size=vocab_size, hidden_size=50,num_classes=20)\n",
        "predicted_labels, loss = mlp.build_graph()\n",
        "train_op = mlp.trainer(loss=loss, learning_rate=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FEOXwFM3tBBx"
      },
      "outputs": [],
      "source": [
        "def load_datasets():\n",
        "  train_data_reader = DataReader(data_path='20news-train-tfidf.txt', batch_size=50, vocab_size=vocab_size)\n",
        "  test_data_reader = DataReader(data_path='20news-test-tfidf.txt', batch_size=50, vocab_size=vocab_size)\n",
        "  return train_data_reader, test_data_reader  \n",
        "\n",
        "def save_parameters(name, value, epoch):\n",
        "  filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
        "  if len(value.shape) == 1:\n",
        "    string_form = ','.join([str(number) for number in value])\n",
        "  else:\n",
        "    string_form = '\\n'.join([','.join([str(number)\n",
        "                                        for number in value[row]])\n",
        "                                          for row in range(value.shape[0])])\n",
        "    \n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(string_form)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QDL_X8INyB10"
      },
      "outputs": [],
      "source": [
        "def restore_parameters(name, epoch):\n",
        "  filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
        "  with open(filename) as f:\n",
        "    lines = f.read().splitlines()\n",
        "  if len(lines) == 1:\n",
        "    value = [float(number) for number in lines[0].split(',')]\n",
        "  else:\n",
        "    value = [[float(number) for number in lines[row].split(',')]\n",
        "              for row in range(len(lines))]\n",
        "  return value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTE9lB14pD9u",
        "outputId": "5dbee44f-8300-49d1-c80b-b3368e296950"
      },
      "outputs": [],
      "source": [
        "with tf.Session() as sess:\n",
        "  train_data_reader, test_data_reader = load_datasets()\n",
        "  step, MAX_STEP = 0 , 100**2\n",
        "\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  while step < MAX_STEP:\n",
        "    train_data, train_labels = train_data_reader.next_batch()\n",
        "    plabels_eval, loss_eval, _ = sess.run(\n",
        "        [predicted_labels, loss, train_op],\n",
        "        feed_dict={\n",
        "            mlp.X: train_data,\n",
        "            mlp.y_real: train_labels\n",
        "        }\n",
        "    )\n",
        "    step+=1\n",
        "    print('step: {}, loss: {}'.format(step, loss_eval))\n",
        "  trainable_variables = tf.trainable_variables()\n",
        "  for variable in trainable_variables:\n",
        "    save_parameters(name=variable.name, value=variable.eval(), epoch=train_data_reader.num_epoch) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSKV2imDmBzs",
        "outputId": "68675226-ff9a-436d-b48e-97f4a220005c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 44\n",
            "Accuracy on test data: 0.7525225703664365\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as sess:\n",
        "  epoch = 44\n",
        "\n",
        "  trainable_variables = tf.trainable_variables()\n",
        "  for variable in trainable_variables:\n",
        "    saved_value = restore_parameters(variable.name, epoch)\n",
        "    assign_op = variable.assign(saved_value)\n",
        "    sess.run(assign_op) \n",
        "\n",
        "  num_true_preds = 0\n",
        "  while True:\n",
        "    test_data, test_labels = test_data_reader.next_batch()\n",
        "    test_plabels_eval = sess.run(predicted_labels,\n",
        "                                 feed_dict={\n",
        "                                     mlp.X: test_data,\n",
        "                                     mlp.y_real: test_labels\n",
        "                                 }\n",
        "                                )\n",
        "    matches = np.equal(test_plabels_eval, test_labels)\n",
        "    num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "    if test_data_reader.batch_id == 0:\n",
        "      break\n",
        "    \n",
        "  print('Epoch:', epoch)\n",
        "  print('Accuracy on test data:', num_true_preds / len(test_data_reader.data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA-NSSUwwozd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "MultiLayers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
